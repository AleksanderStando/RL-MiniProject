{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Zależności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.18.3)\n",
      "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (2.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/aleksander/.local/lib/python3.6/site-packages (from tensorflow==2.3.1) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.3.1) (0.30.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.28.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (3.11.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (0.9.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (3.2.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /usr/lib/python3/dist-packages (from gym) (5.1.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/aleksander/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.14.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2018.1.18)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Losowy agent w środowisku OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:80.0\n",
      "Episode:2 Score:75.0\n",
      "Episode:3 Score:110.0\n",
      "Episode:4 Score:30.0\n",
      "Episode:5 Score:120.0\n",
      "Episode:6 Score:240.0\n",
      "Episode:7 Score:155.0\n",
      "Episode:8 Score:135.0\n",
      "Episode:9 Score:430.0\n",
      "Episode:10 Score:45.0\n",
      "Episode:11 Score:50.0\n",
      "Episode:12 Score:155.0\n",
      "Episode:13 Score:155.0\n",
      "Episode:14 Score:50.0\n",
      "Episode:15 Score:50.0\n",
      "Episode:16 Score:135.0\n",
      "Episode:17 Score:75.0\n",
      "Episode:18 Score:80.0\n",
      "Episode:19 Score:120.0\n",
      "Episode:20 Score:90.0\n",
      "Episode:21 Score:160.0\n",
      "Episode:22 Score:20.0\n",
      "Episode:23 Score:155.0\n",
      "Episode:24 Score:30.0\n",
      "Episode:25 Score:80.0\n",
      "Episode:26 Score:320.0\n",
      "Episode:27 Score:290.0\n",
      "Episode:28 Score:135.0\n",
      "Episode:29 Score:20.0\n",
      "Episode:30 Score:270.0\n",
      "Episode:31 Score:300.0\n",
      "Episode:32 Score:240.0\n",
      "Episode:33 Score:300.0\n",
      "Episode:34 Score:120.0\n",
      "Episode:35 Score:135.0\n",
      "Episode:36 Score:105.0\n",
      "Episode:37 Score:105.0\n",
      "Episode:38 Score:210.0\n",
      "Episode:39 Score:120.0\n",
      "Episode:40 Score:260.0\n",
      "Episode:41 Score:80.0\n",
      "Episode:42 Score:75.0\n",
      "Episode:43 Score:120.0\n",
      "Episode:44 Score:95.0\n",
      "Episode:45 Score:140.0\n",
      "Episode:46 Score:110.0\n",
      "Episode:47 Score:105.0\n",
      "Episode:48 Score:150.0\n",
      "Episode:49 Score:180.0\n",
      "Episode:50 Score:60.0\n",
      "Episode:51 Score:150.0\n",
      "Episode:52 Score:85.0\n",
      "Episode:53 Score:155.0\n",
      "Episode:54 Score:135.0\n",
      "Episode:55 Score:120.0\n",
      "Episode:56 Score:65.0\n",
      "Episode:57 Score:125.0\n",
      "Episode:58 Score:65.0\n",
      "Episode:59 Score:110.0\n",
      "Episode:60 Score:215.0\n",
      "Episode:61 Score:105.0\n",
      "Episode:62 Score:155.0\n",
      "Episode:63 Score:315.0\n",
      "Episode:64 Score:130.0\n",
      "Episode:65 Score:105.0\n",
      "Episode:66 Score:50.0\n",
      "Episode:67 Score:155.0\n",
      "Episode:68 Score:5.0\n",
      "Episode:69 Score:115.0\n",
      "Episode:70 Score:460.0\n",
      "Episode:71 Score:240.0\n",
      "Episode:72 Score:185.0\n",
      "Episode:73 Score:110.0\n",
      "Episode:74 Score:90.0\n",
      "Episode:75 Score:105.0\n",
      "Episode:76 Score:105.0\n",
      "Episode:77 Score:240.0\n",
      "Episode:78 Score:440.0\n",
      "Episode:79 Score:135.0\n",
      "Episode:80 Score:255.0\n",
      "Episode:81 Score:80.0\n",
      "Episode:82 Score:90.0\n",
      "Episode:83 Score:235.0\n",
      "Episode:84 Score:75.0\n",
      "Episode:85 Score:105.0\n",
      "Episode:86 Score:105.0\n",
      "Episode:87 Score:105.0\n",
      "Episode:88 Score:210.0\n",
      "Episode:89 Score:315.0\n",
      "Episode:90 Score:35.0\n",
      "Episode:91 Score:205.0\n",
      "Episode:92 Score:220.0\n",
      "Episode:93 Score:440.0\n",
      "Episode:94 Score:120.0\n",
      "Episode:95 Score:65.0\n",
      "Episode:96 Score:60.0\n",
      "Episode:97 Score:155.0\n",
      "Episode:98 Score:135.0\n",
      "Episode:99 Score:105.0\n",
      "Episode:100 Score:110.0\n",
      "Avg score: 145.65\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "total_score = 0\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "    total_score += score\n",
    "print('Avg score: {}'.format(str(total_score/episodes)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sieć neuronowa w Kerasie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    input_shape = (3, height, width, channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Graph().as_default()\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Agent w Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "  708/10000: episode: 1, duration: 16.068s, episode steps: 708, steps per second:  44, episode reward: 110.000, mean reward:  0.155 [ 0.000, 30.000], mean action: 2.376 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      " 1379/10000: episode: 2, duration: 470.332s, episode steps: 671, steps per second:   1, episode reward: 105.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 2.516 [0.000, 5.000],  loss: 16.554533, mean_q: 8.911127, mean_eps: 0.892945\n",
      " 2295/10000: episode: 3, duration: 1107.539s, episode steps: 916, steps per second:   1, episode reward: 120.000, mean reward:  0.131 [ 0.000, 30.000], mean action: 2.563 [0.000, 5.000],  loss: 1.398586, mean_q: 8.688404, mean_eps: 0.834715\n",
      " 2866/10000: episode: 4, duration: 669.088s, episode steps: 571, steps per second:   1, episode reward:  5.000, mean reward:  0.009 [ 0.000,  5.000], mean action: 2.588 [0.000, 5.000],  loss: 0.611059, mean_q: 8.642722, mean_eps: 0.767800\n",
      " 3446/10000: episode: 5, duration: 682.775s, episode steps: 580, steps per second:   1, episode reward: 65.000, mean reward:  0.112 [ 0.000, 20.000], mean action: 2.474 [0.000, 5.000],  loss: 0.397734, mean_q: 8.430221, mean_eps: 0.716005\n",
      " 4104/10000: episode: 6, duration: 785.878s, episode steps: 658, steps per second:   1, episode reward: 110.000, mean reward:  0.167 [ 0.000, 30.000], mean action: 2.255 [0.000, 5.000],  loss: 0.541173, mean_q: 7.920720, mean_eps: 0.660295\n",
      " 4746/10000: episode: 7, duration: 721.684s, episode steps: 642, steps per second:   1, episode reward: 185.000, mean reward:  0.288 [ 0.000, 30.000], mean action: 2.421 [0.000, 5.000],  loss: 0.793009, mean_q: 8.641563, mean_eps: 0.601795\n",
      " 5345/10000: episode: 8, duration: 619.563s, episode steps: 599, steps per second:   1, episode reward: 80.000, mean reward:  0.134 [ 0.000, 20.000], mean action: 2.374 [0.000, 5.000],  loss: 0.715801, mean_q: 8.344586, mean_eps: 0.545950\n",
      " 5855/10000: episode: 9, duration: 560.252s, episode steps: 510, steps per second:   1, episode reward: 30.000, mean reward:  0.059 [ 0.000, 15.000], mean action: 2.439 [0.000, 5.000],  loss: 0.304668, mean_q: 8.178548, mean_eps: 0.496045\n",
      " 6406/10000: episode: 10, duration: 653.562s, episode steps: 551, steps per second:   1, episode reward: 105.000, mean reward:  0.191 [ 0.000, 30.000], mean action: 2.457 [0.000, 5.000],  loss: 0.495140, mean_q: 8.449864, mean_eps: 0.448300\n",
      " 7425/10000: episode: 11, duration: 1203.597s, episode steps: 1019, steps per second:   1, episode reward: 230.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.496 [0.000, 5.000],  loss: 0.666467, mean_q: 8.461721, mean_eps: 0.377650\n",
      " 7995/10000: episode: 12, duration: 661.467s, episode steps: 570, steps per second:   1, episode reward: 120.000, mean reward:  0.211 [ 0.000, 30.000], mean action: 2.489 [0.000, 5.000],  loss: 0.524754, mean_q: 8.096800, mean_eps: 0.306145\n",
      " 9037/10000: episode: 13, duration: 1082.985s, episode steps: 1042, steps per second:   1, episode reward: 165.000, mean reward:  0.158 [ 0.000, 30.000], mean action: 2.417 [0.000, 5.000],  loss: 0.410317, mean_q: 8.752953, mean_eps: 0.233605\n",
      " 9582/10000: episode: 14, duration: 565.617s, episode steps: 545, steps per second:   1, episode reward: 20.000, mean reward:  0.037 [ 0.000, 10.000], mean action: 2.624 [0.000, 5.000],  loss: 0.100831, mean_q: 8.229603, mean_eps: 0.162190\n",
      "done, took 10232.644 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f94502cee80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 105.000, steps: 674\n",
      "Episode 2: reward: 140.000, steps: 926\n",
      "Episode 3: reward: 70.000, steps: 613\n",
      "Episode 4: reward: 75.000, steps: 564\n",
      "Episode 5: reward: 395.000, steps: 901\n",
      "Episode 6: reward: 20.000, steps: 661\n",
      "Episode 7: reward: 410.000, steps: 1131\n",
      "Episode 8: reward: 105.000, steps: 755\n",
      "Episode 9: reward: 170.000, steps: 846\n",
      "Episode 10: reward: 245.000, steps: 704\n",
      "Episode 11: reward: 270.000, steps: 894\n",
      "Episode 12: reward: 100.000, steps: 665\n",
      "Episode 13: reward: 130.000, steps: 894\n",
      "Episode 14: reward: 55.000, steps: 395\n",
      "Episode 15: reward: 165.000, steps: 698\n",
      "Episode 16: reward: 90.000, steps: 642\n",
      "Episode 17: reward: 100.000, steps: 655\n",
      "Episode 18: reward: 420.000, steps: 1344\n",
      "Episode 19: reward: 560.000, steps: 984\n",
      "Episode 20: reward: 80.000, steps: 511\n",
      "Episode 21: reward: 135.000, steps: 656\n",
      "Episode 22: reward: 240.000, steps: 916\n",
      "Episode 23: reward: 170.000, steps: 658\n",
      "Episode 24: reward: 345.000, steps: 1191\n",
      "Episode 25: reward: 60.000, steps: 544\n",
      "Episode 26: reward: 65.000, steps: 810\n",
      "Episode 27: reward: 145.000, steps: 633\n",
      "Episode 28: reward: 115.000, steps: 654\n",
      "Episode 29: reward: 105.000, steps: 403\n",
      "Episode 30: reward: 865.000, steps: 1962\n",
      "Episode 31: reward: 155.000, steps: 1118\n",
      "Episode 32: reward: 90.000, steps: 467\n",
      "Episode 33: reward: 295.000, steps: 781\n",
      "Episode 34: reward: 785.000, steps: 1651\n",
      "Episode 35: reward: 80.000, steps: 559\n",
      "Episode 36: reward: 395.000, steps: 693\n",
      "Episode 37: reward: 75.000, steps: 511\n",
      "Episode 38: reward: 95.000, steps: 892\n",
      "Episode 39: reward: 55.000, steps: 394\n",
      "Episode 40: reward: 100.000, steps: 809\n",
      "Episode 41: reward: 165.000, steps: 790\n",
      "Episode 42: reward: 35.000, steps: 594\n",
      "Episode 43: reward: 45.000, steps: 598\n",
      "Episode 44: reward: 515.000, steps: 1115\n",
      "Episode 45: reward: 50.000, steps: 390\n",
      "Episode 46: reward: 110.000, steps: 386\n",
      "Episode 47: reward: 20.000, steps: 519\n",
      "Episode 48: reward: 255.000, steps: 891\n",
      "Episode 49: reward: 70.000, steps: 799\n",
      "Episode 50: reward: 90.000, steps: 622\n",
      "Episode 51: reward: 135.000, steps: 811\n",
      "Episode 52: reward: 130.000, steps: 669\n",
      "Episode 53: reward: 70.000, steps: 632\n",
      "Episode 54: reward: 165.000, steps: 925\n",
      "Episode 55: reward: 95.000, steps: 791\n",
      "Episode 56: reward: 35.000, steps: 526\n",
      "Episode 57: reward: 10.000, steps: 577\n",
      "Episode 58: reward: 130.000, steps: 875\n",
      "Episode 59: reward: 215.000, steps: 975\n",
      "Episode 60: reward: 525.000, steps: 1305\n",
      "Episode 61: reward: 115.000, steps: 635\n",
      "Episode 62: reward: 355.000, steps: 932\n",
      "Episode 63: reward: 265.000, steps: 1173\n",
      "Episode 64: reward: 435.000, steps: 1273\n",
      "Episode 65: reward: 65.000, steps: 531\n",
      "Episode 66: reward: 205.000, steps: 687\n",
      "Episode 67: reward: 55.000, steps: 382\n",
      "Episode 68: reward: 195.000, steps: 1014\n",
      "Episode 69: reward: 20.000, steps: 425\n",
      "Episode 70: reward: 340.000, steps: 1207\n",
      "Episode 71: reward: 210.000, steps: 1044\n",
      "Episode 72: reward: 75.000, steps: 412\n",
      "Episode 73: reward: 30.000, steps: 381\n",
      "Episode 74: reward: 100.000, steps: 612\n",
      "Episode 75: reward: 125.000, steps: 695\n",
      "Episode 76: reward: 345.000, steps: 1021\n",
      "Episode 77: reward: 45.000, steps: 398\n",
      "Episode 78: reward: 75.000, steps: 400\n",
      "Episode 79: reward: 170.000, steps: 993\n",
      "Episode 80: reward: 75.000, steps: 618\n",
      "Episode 81: reward: 65.000, steps: 585\n",
      "Episode 82: reward: 25.000, steps: 524\n",
      "Episode 83: reward: 185.000, steps: 839\n",
      "Episode 84: reward: 200.000, steps: 898\n",
      "Episode 85: reward: 195.000, steps: 898\n",
      "Episode 86: reward: 45.000, steps: 660\n",
      "Episode 87: reward: 290.000, steps: 1159\n",
      "Episode 88: reward: 65.000, steps: 519\n",
      "Episode 89: reward: 75.000, steps: 620\n",
      "Episode 90: reward: 150.000, steps: 662\n",
      "Episode 91: reward: 430.000, steps: 811\n",
      "Episode 92: reward: 30.000, steps: 386\n",
      "Episode 93: reward: 265.000, steps: 797\n",
      "Episode 94: reward: 40.000, steps: 408\n",
      "Episode 95: reward: 50.000, steps: 605\n",
      "Episode 96: reward: 100.000, steps: 625\n",
      "Episode 97: reward: 215.000, steps: 879\n",
      "Episode 98: reward: 65.000, steps: 519\n",
      "Episode 99: reward: 125.000, steps: 656\n",
      "Episode 100: reward: 5.000, steps: 509\n",
      "168.6\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Zapis i odczyt agenta z pamięci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/SpaceInvaders/10k-dqn-weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/SpaceInvaders/10k-dqn-weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1e3fdc59c23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "del dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
